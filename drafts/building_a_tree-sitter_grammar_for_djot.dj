---
title: "Creating a Tree-sitter grammar for a subset of Djot"
tags: ["Djot", "Tree-sitter", "C"]
---

One of my favorite features in Neovim is the Tree-sitter integration.
It allows for fast syntax highlighting that works well even in an error state (often the case when you're editing code), and it has additional semantics (you can differentiate between function parameters and local variables).

With [nvim-treesitter-textobjects][] you can also jump between nodes (such as `]c` to jump to next class) or target deletion (`cif` to delete the function body and enter insert mode).
An amazing feature as it works across languages, no matter how the look like.

But, you might wonder how does Tree-sitter work?
How do you create a Tree-sitter parser for a language?

I started thinking about this and before I knew it I was trying to make my own parser for [Djot][] (a markup language similar to Markdown).
There are some good tutorials on how to get started, but not on some mare advanced things.

Specifically, I had some questions on how to use an external scanner, and here's a post on how and why you can use an external scanner in your Tree-sitter parser.

[nvim-treesitter-textobjects]: https://github.com/nvim-treesitter/nvim-treesitter-textobjects

# Our subset

For the purpose of this blog post, we'll implement a small subset of [Djot][]:

1. Paragraphs
1. Divs
1. Emphasis

This allows us to parse markup like this:

```djot
This is a
multiline _paragraph_

::: div-class
This is a paragraph inside a div
:::
```

At first blush, this seems like it's too simple to require anything more complex other than some simple grammar rules, but later on we'll see that even these simple rules contain complicated edge-cases.

# Simple beginnings

The point of this post isn't to go through how the Tree-sitter grammar description in `grammar.js` works.
The [Tree-sitter docs][] goes through how to get started pretty well.
I named the project `sdjot` and this is the `grammar.js` we'll start with:

[Tree-sitter docs]: https://tree-sitter.github.io/tree-sitter/creating-parsers

```js
module.exports = grammar({
  name: "sdjot",

  // Skip carriage returns.
  // We could skip spaces here as well, but the actual markup language
  // has significant spaces in some places, so let's remove them here too.
  extras: (_) => ["\r"],

  rules: {
    document: ($) => repeat($._block),

    // All blocks should end with a newline, but we can also eat newlines directly.
    _block: ($) => choice($.div, $.paragraph, "\n"),

    // A div contains other blocks.
    div: ($) =>
      prec.left(seq($.div_marker, "\n", repeat($._block), $.div_marker, "\n")),
    div_marker: (_) => ":::",

    // A paragraph contains inline content and is terminated by a blankline
    // (two newlines in a row).
    paragraph: ($) => seq(repeat1(seq($._inline, "\n")), "\n"),

    // The markup parser could separate block and inline parsing into separate steps,
    // but we'll do everything in one parser.
    _inline: ($) => repeat1(choice($.emphasis, /[^\n]/)),
    emphasis: ($) => prec.left(seq("_", $._inline, "_")),
  },
});
```

It recognizes paragraphs with text and emphasis, and it identifies divs with begin and end tags.

We can create an `example-file` with these contents:

```djot
:::
A paragraph _with emphasis_ inside a div

:::
```

And parse it with the `tree-sitter` cli:

```fish
$ tree-sitter parse example-file
(document [0, 0] - [5, 0]
  (div [0, 0] - [4, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [3, 0]
      (emphasis [1, 12] - [1, 27]))
    (div_marker [3, 0] - [3, 3])))
```

Et voilÃ !

## Missing features

But I told you it wasn't supposed to be this easy, and there are two features missing from our parser:

1. There can be an arbitrary number of `:`, allowing divs to be nested.
1. Closing a div should close other open blocks (divs and paragraphs in our case).

In essence, we need to be able to parse this:

```djot
:::
Top-level div

::::
A paragraph inside a second div,
both closed when the top-level div is closed
:::
```

This is... Complicated.

Sure, we can work around the varying levels of `:` with something hacky like enumerating the number of colons, with something like this:

```js
div: ($) => choice($._div3, $._div4, $._div5, $._div6, $._div7, $._div8),
_div3: ($) => seq(/:{3}/, $._inside_div, /:{3}/, "\n"),
_div4: ($) => seq(/:{4}/, $._inside_div, /:{4}/, "\n"),
_div5: ($) => seq(/:{5}/, $._inside_div, /:{5}/, "\n"),
_div6: ($) => seq(/:{6}/, $._inside_div, /:{6}/, "\n"),
_div7: ($) => seq(/:{7}/, $._inside_div, /:{7}/, "\n"),
_div8: ($) => seq(/:{8}/, $._inside_div, /:{8}/, "\n"),
_inside_div: ($) => prec.left("\n", repeat($._block)),
```

But it's not _neat_, and automatically closing matching blocks is much harder (to my brain it seems impossible, but I'm no expert).

With an external scanner we can do this (and more).

# External scanner

A Tree-sitter parser is actually a C program.
The grammar we've seen has been described in JavaScript, but it's only used as a description to generate the parser in C.
If you're a masochist, you can take a look at it in `src/parser.c` after running `tree-sitter generate`.

An external scanner is just some custom C code that's inserted into the parser, and it allows us to override the parser precedence, keep track of a context state, or whatever else we might need or want to do.

To get started the [official docs][external-scanners] was pretty good.
Basically you need to:

1. Create a `src/scanner.c` and include it in `binding.gyp` `bindings/rust/build.rs`.
1. Setup `externals` tokens in `grammar.js` and a matching C enum in `scanner.c`.
1. Define and implement five C functions.

Let's take a look.

## Div markers closes open paragraphs

Let's start by closing a paragraph early when a `:::` is encountered.
This is simpler because we don't have to store any state.

When parsing `$.paragraph` we'll give the parser a choice between ending the paragraph on a newline or on our new `$._close_paragraph` token:


```js
paragraph: ($) =>
  seq(repeat1(seq($._inline, "\n")), choice("\n", $._close_paragraph)),
```

`$._close_paragraph` is handled by the external scanner, which is specified using the `externals` field:

```js
externals: ($) => [$._close_paragraph],
```

Now let's turn our attention to `src/scanner.c`.

The tokens in `externals` gets assigned an incremented number, starting from 0...
Just like an enum in C!

```c
// We only have a single element right now, but keep in mind that the order
// must match the `externals` array in `grammar.js`.
typedef enum { CLOSE_PARAGRAPH } TokenType;
```

The five functions we need to implement are these:

```c
// You should replace `sdjot` with whatever project name you chose.
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  // All the scanning goes here.
  return false;
}

// If we need to allocate/deallocate state, we do it in these functions.
void *tree_sitter_sdjot_external_scanner_create() { return NULL; }
void tree_sitter_sdjot_external_scanner_destroy(void *payload) {}

// If we have state, we should load and save it in these functions.
unsigned tree_sitter_sdjot_external_scanner_serialize(void *payload,
                                                      char *buffer) {
  return 0;
}
void tree_sitter_sdjot_external_scanner_deserialize(void *payload, char *buffer,
                                                    unsigned length) {}
```

Because we won't use any state, we'll only have to update the `scan` function.

What you're supposed to do is check `valid_symbols` for the tokens we can return at any point in time, and return `true` if any was found:

```c
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  if (valid_symbols[CLOSE_PARAGRAPH] && parse_close_paragraph(lexer)) {
    return true;
  }
  return false;
}
```

The matched token should be stored in `lexer->result_symbol`c:

```c
bool parse_close_paragraph(TSLexer *lexer) {
  // Mark the end before advancing so that the CLOSE_PARAGRAPH token doesn't
  // consume any characters.
  lexer->mark_end(lexer);

  uint8_t colons = consume_chars(lexer, ':');
  if (colons == 3) {
    lexer->result_symbol = CLOSE_PARAGRAPH;
    return true;
  } else {
    return false;
  }
}
```

Note that the resulting token will mark any symbol we advance over as owned by that token.
So `:::` would be marked as `_close_paragraph` (which will be ignored by the output since it begins with an underscore), instead of `div_marker`.
To stop prevent this, we turn `_close_paragraph` into a zero-width token by marking the end before advancing the lexer.

How do we advance the lexer?
We call `lexer->advance`c:

```c
uint8_t consume_chars(TSLexer *lexer, char c) {
  uint8_t count = 0;
  while (lexer->lookahead == c) {
    lexer->advance(lexer, false);
    ++count;
  }
  return count;
}
```

This is almost all we can do with the lexer.
We only process one character at a time, cannot look behind, and our only tool to look ahead is to `mark_end` at the correct place.
(We can also query the current column position.)

With this we have a working external scanner and div tags now close paragraphs:

```
:::
A paragraph inside a div
:::
```

```fish
$ tree-sitter parse example-file
(document [0, 0] - [4, 0]
  (div [0, 0] - [3, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [2, 0])
    (div_marker [2, 0] - [2, 3])))
```

(This may not be the best design, having to parse div markers multiple times, but it gets the job done.)

## Tracking nested blocks

To automatically close other open blocks we need to add some context to our parser, which means we'll need state management.

The small subset we're using for the blog is only concerned with divs because it would be a terribly long post otherwise, but I'll try to implement this in a general manner, to be more indicative of a real-world parser.

I like Torvalds take on focusing on data structures:

> I will, in fact, claim that the difference between a bad programmer and a good one
> is whether he considers his code or his data structures more important.
> Bad programmers worry about the code.
> Good programmers worry about data structures and their relationships.
> ^ Linus Torvalds

I might not be a "good programmer", but if you pretend enough maybe you'll accidentally turn into one?

TODO

(alloc and array are new features!)

1. Use ts_ from "tree_sitter/alloc.h"
1. Use array macros from "tree_sitter/array.h"
1. Should detect if we're in an error detection mode


# Adding highlights

## Using our grammar with Neovim

[external-scanners]: https://tree-sitter.github.io/tree-sitter/creating-parsers#external-scanners
