---
title: "Creating a Tree-sitter grammar for a subset of Djot"
tags: ["Djot", "Tree-sitter", "C"]
---

One of my favorite features in Neovim is the Tree-sitter integration.
It allows for fast syntax highlighting that works well even in an error state (often the case when you're editing code), and it has additional semantics (you can differentiate between function parameters and local variables).

With [nvim-treesitter-textobjects][] you can also jump between nodes (such as `]c` to jump to next class) or target deletion (`cif` to delete the function body and enter insert mode).
An amazing feature as it works across languages, no matter how the look like.

But, you might wonder how does Tree-sitter work?
How do you create a Tree-sitter parser for a language?

I started thinking about this and before I knew it I was trying to make my own parser for [Djot][] (a markup language similar to Markdown).
There are some good tutorials on how to get started, but not on some mare advanced things.

Specifically, I had some questions on how to use an external scanner, and here's a post on how and why you can use an external scanner in your Tree-sitter parser.

[nvim-treesitter-textobjects]: https://github.com/nvim-treesitter/nvim-treesitter-textobjects

# Our subset

For the purpose of this blog post, we'll implement a small subset of [Djot][]:

1. Paragraphs
1. Divs
1. Emphasis

This allows us to parse markup like this:

```djot
This is a
multiline _paragraph_

::: div-class
This is a paragraph inside a div
:::
```

At first blush, this seems like it's too simple to require anything more complex other than some simple grammar rules, but later on we'll see that even these simple rules contain complicated edge-cases.

# Simple beginnings

The point of this post isn't to go through how the Tree-sitter grammar description in `grammar.js` works.
The [Tree-sitter docs][] goes through how to get started pretty well.
I named the project `sdjot` and this is the `grammar.js` we'll start with:

[Tree-sitter docs]: https://tree-sitter.github.io/tree-sitter/creating-parsers

```js
module.exports = grammar({
  name: "sdjot",

  // Skip carriage returns.
  // We could skip spaces here as well, but the actual markup language
  // has significant spaces in some places, so let's remove them here too.
  extras: (_) => ["\r"],

  rules: {
    document: ($) => repeat($._block),

    // All blocks should end with a newline, but we can also eat newlines directly.
    _block: ($) => choice($.div, $.paragraph, "\n"),

    // A div contains other blocks.
    div: ($) =>
      prec.left(seq($.div_marker, "\n", repeat($._block), $.div_marker, "\n")),
    div_marker: (_) => ":::",

    // A paragraph contains inline content and is terminated by a blankline
    // (two newlines in a row).
    paragraph: ($) => seq(repeat1(seq($._inline, "\n")), "\n"),

    // The markup parser could separate block and inline parsing into separate steps,
    // but we'll do everything in one parser.
    _inline: ($) => repeat1(choice($.emphasis, /[^\n]/)),
    emphasis: ($) => prec.left(seq("_", $._inline, "_")),
  },
});
```

It recognizes paragraphs with text and emphasis, and it identifies divs with begin and end tags.

We can create an `example-file` with these contents:

```djot
:::
A paragraph _with emphasis_ inside a div

:::
```

And parse it with the `tree-sitter` cli:

```fish
$ tree-sitter parse example-file
(document [0, 0] - [5, 0]
  (div [0, 0] - [4, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [3, 0]
      (emphasis [1, 12] - [1, 27]))
    (div_marker [3, 0] - [3, 3])))
```

Et voilÃ !

## Missing features

But I told you it wasn't supposed to be this easy, and there are two features missing from our parser:

1. There can be an arbitrary number of `:`, allowing divs to be nested.
1. Closing a div should close other open blocks (divs and paragraphs in our case).

In essence, we need to be able to parse this:

```djot
:::
Top-level div

::::
A paragraph inside a second div,
both closed when the top-level div is closed
:::
```

This is... Complicated.

Sure, we can work around the varying levels of `:` with something hacky like enumerating the number of colons, with something like this:

```js
div: ($) => choice($._div3, $._div4, $._div5, $._div6, $._div7, $._div8),
_div3: ($) => seq(/:{3}/, $._inside_div, /:{3}/, "\n"),
_div4: ($) => seq(/:{4}/, $._inside_div, /:{4}/, "\n"),
_div5: ($) => seq(/:{5}/, $._inside_div, /:{5}/, "\n"),
_div6: ($) => seq(/:{6}/, $._inside_div, /:{6}/, "\n"),
_div7: ($) => seq(/:{7}/, $._inside_div, /:{7}/, "\n"),
_div8: ($) => seq(/:{8}/, $._inside_div, /:{8}/, "\n"),
_inside_div: ($) => prec.left("\n", repeat($._block)),
```

But it's not _neat_, and automatically closing matching blocks is much harder (to my brain it seems impossible, but I'm no expert).

With an external scanner we can do this (and more).

# External scanner

A Tree-sitter parser is actually a C program.
The grammar we've seen has been described in JavaScript, but it's only used as a description to generate the parser in C.
If you're a masochist, you can take a look at it in `src/parser.c` after running `tree-sitter generate`.

An external scanner is just some custom C code that's inserted into the parser, and it allows us to override the parser precedence, keep track of a context state, or whatever else we might need or want to do.

To get started the [official docs][external-scanners] was pretty good.
Basically you need to:

1. Create a `src/scanner.c` and include it in `binding.gyp` `bindings/rust/build.rs`.
1. Setup `externals` tokens in `grammar.js` and a matching C enum in `scanner.c`.
1. Define and implement five C functions.

Let's take a look.

## Div markers closes open paragraphs

Let's start by closing a paragraph early when a `:::` is encountered.
This is simpler because we don't have to store any state.

When parsing `$.paragraph` we'll give the parser a choice between ending the paragraph on a newline or on our new `$._close_paragraph` token:


```js
paragraph: ($) =>
  seq(repeat1(seq($._inline, "\n")), choice("\n", $._close_paragraph)),
```

`$._close_paragraph` is handled by the external scanner, which is specified using the `externals` field:

```js
externals: ($) => [$._close_paragraph],
```

Now let's turn our attention to `src/scanner.c`.

The tokens in `externals` gets assigned an incremented number, starting from 0...
Just like an enum in C!

```c
// We only have a single element right now, but keep in mind that the order
// must match the `externals` array in `grammar.js`.
typedef enum { CLOSE_PARAGRAPH } TokenType;
```

The five functions we need to implement are these:

```c
// You should replace `sdjot` with whatever project name you chose.
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  // All the scanning goes here.
  return false;
}

// If we need to allocate/deallocate state, we do it in these functions.
void *tree_sitter_sdjot_external_scanner_create() { return NULL; }
void tree_sitter_sdjot_external_scanner_destroy(void *payload) {}

// If we have state, we should load and save it in these functions.
unsigned tree_sitter_sdjot_external_scanner_serialize(void *payload,
                                                      char *buffer) {
  return 0;
}
void tree_sitter_sdjot_external_scanner_deserialize(void *payload, char *buffer,
                                                    unsigned length) {}
```

Because we won't use any state, we'll only have to update the `scan` function.

What you're supposed to do is check `valid_symbols` for the tokens we can return at any point in time, and return `true` if any was found:

```c
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  if (valid_symbols[CLOSE_PARAGRAPH] && parse_close_paragraph(lexer)) {
    return true;
  }
  return false;
}
```

The matched token should be stored in `lexer->result_symbol`c:

```c
bool parse_close_paragraph(TSLexer *lexer) {
  // Mark the end before advancing so that the CLOSE_PARAGRAPH token doesn't
  // consume any characters.
  lexer->mark_end(lexer);

  uint8_t colons = consume_chars(lexer, ':');
  if (colons == 3) {
    lexer->result_symbol = CLOSE_PARAGRAPH;
    return true;
  } else {
    return false;
  }
}
```

Note that the resulting token will mark any symbol we advance over as owned by that token.
So `:::` would be marked as `_close_paragraph` (which will be ignored by the output since it begins with an underscore), instead of `div_marker`.
To stop prevent this, we turn `_close_paragraph` into a zero-width token by marking the end before advancing the lexer.

How do we advance the lexer?
We call `lexer->advance`c:

```c
uint8_t consume_chars(TSLexer *lexer, char c) {
  uint8_t count = 0;
  while (lexer->lookahead == c) {
    lexer->advance(lexer, false);
    ++count;
  }
  return count;
}
```

This is almost all we can do with the lexer.
We only process one character at a time, cannot look behind, and our only tool to look ahead is to `mark_end` at the correct place.
(We can also query the current column position.)

With this we have a working external scanner and div tags now close paragraphs:

```
:::
A paragraph inside a div
:::
```

```fish
$ tree-sitter parse example-file
(document [0, 0] - [4, 0]
  (div [0, 0] - [3, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [2, 0])
    (div_marker [2, 0] - [2, 3])))
```

(This may not be the best design, having to parse div markers multiple times, but it gets the job done.)

## Tracking nested blocks

To automatically close other open blocks we need to add some context to our parser, which means we'll need state management.

The small subset we're using for the blog is only concerned with divs because it would be a terribly long post otherwise, but I'll try to implement this in a general manner, to be more indicative of a real-world parser.


TODO

(alloc and array are new features!)

1. Use ts_ from "tree_sitter/alloc.h"
1. Use array macros from "tree_sitter/array.h"
1. Should detect if we're in an error detection mode


    *   *    *

## A simple scanner

First, lets ignore nested blocks and try to only replicate matching starting and ending `:::`.

## Arbitrary levels and nested blocks

Something like this may parse...? But I dunno...

```c
static Block *find_block(Scanner *s, BlockType type, uint8_t level) {
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    if (b->type == type && b->level == level) {
      return b;
    }
  }
  return NULL;
}

static void dump_stach(Scanner *s) {
  printf("=== Stack size: %zu\n", s->open_blocks.size);
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    printf("  %d\n", b->level);
  }
  printf("=== Stack\n");
}

// Remove blocks until 'b' is reached.
// Is inclusive, so will free 'b'!
static void remove_blocks_until(Scanner *s, Block *b) {
  for (;;) {
    Block *top = peek_block(s);
    bool found = top == b;
    // printf("-> removing block %d\n", top->level);
    // TODO should issue a $._block_close token
    // instead of just removing it here...
    // So we need to set some kind of state here instead
    // and pop the topmost block until we reach
    // our target block.
    pop_block(s);
    dump_stach(s);

    // Should always work, size check is just a safeguard.
    if (found || s->open_blocks.size == 0) {
      return;
    }
  }
}

static bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  uint8_t colons = 0;
  while (lexer->lookahead == ':') {
    lexer->advance(lexer, false);
    ++colons;
  }

  if (colons < 3) {
    return false;
  }

  // The context could either be a start or an end token.
  // To figure out which we should do, we search through the entire
  // block stack to find if there's an open block somewhere
  // with the same number of colons.
  // If there is, we should close that one, otherwise
  // we start a new div.
  Block *existing = find_block(s, DIV, colons);
  if (existing) {
    lexer->result_symbol = DIV_END;
    remove_blocks_until(s, existing);

    return true;
  } else {
    Block *b = malloc(sizeof(Block));
    b->level = colons;
    b->type = DIV;

    push_block(s, b);
    lexer->result_symbol = DIV_START;

    return true;
  }

  // if (has_block(s)) {
  //   Block *current = peek_block(s);
  //   // We found a matching closing div tag
  //   if (current->type == DIV && current->level == colons) {
  //     pop_block(s);
  //     lexer->result_symbol = DIV_END;
  //     return true;
  //   }
  // }
}
```

## Closing blocks

This can close divs, but cannot close paragraphs:

```js
alias($._block_close, $.block_close),
optional(alias($._div_end, $.div_marker_end))
```

```c
#include "tree_sitter/parser.h"
#include <assert.h>
#include <stdio.h>
#include <string.h>

// Maybe we should implement a growable stack or something,
// but this is probably fine.
#define STACK_SIZE 512

typedef enum { BLOCK_CLOSE, DIV_START, DIV_END, ERROR, UNUSED } TokenType;

typedef enum {
  DIV,
} BlockType;

typedef struct {
  // Level can be either indentation or number of opening/ending symbols.
  // Or it may also be unused.
  uint8_t level;
  BlockType type;
} Block;

typedef struct {
  struct {
    size_t size;
    Block *items[STACK_SIZE];
  } open_blocks;

  // How many $._close_block we should output right now?
  uint8_t blocks_to_close;
  // After we have closed all blocks, what symbol should we output?
  TokenType block_close_final_token;
  uint8_t final_token_width;
} Scanner;

static void push_block(Scanner *s, uint8_t level, BlockType type) {
  Block *b = malloc(sizeof(Block));
  b->level = level;
  b->type = type;
  s->open_blocks.items[s->open_blocks.size++] = b;
}

static void pop_block(Scanner *s) {
  if (s->open_blocks.size > 0) {
    printf("POP\n");
    free(s->open_blocks.items[--s->open_blocks.size]);
  }
}

static bool has_block(Scanner *s) { return s->open_blocks.size > 0; }

static Block *peek_block(Scanner *s) {
  assert(s->open_blocks.size > 0);
  return s->open_blocks.items[s->open_blocks.size - 1];
}

// How many blocks from the top of the stack can we find a matching block?
// If it's directly on the top, returns 1.
// If it cannot be found, returns 0.
static size_t number_of_blocks_from_top(Scanner *s, BlockType type,
                                        uint8_t level) {
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    if (b->type == type && b->level == level) {
      return s->open_blocks.size - i;
    }
  }
  return 0;
}

// Remove 'count' blocks from the stack, freeing them.
static void remove_blocks(Scanner *s, size_t count) {
  while (count-- > 0) {
    pop_block(s);
  }
}

static void dump(Scanner *s) {
  printf("=== Stack size: %zu\n", s->open_blocks.size);
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    printf("  %d\n", b->level);
  }
  printf("---\n");
  printf("  blocks_to_close: %d\n", s->blocks_to_close);
  printf("  final_token_width: %d\n", s->final_token_width);
  printf("  block_close_final_token: %u\n", s->block_close_final_token);
  printf("===\n");
}

static void close_blocks(Scanner *s, TSLexer *lexer, size_t count,
                         TokenType final, uint8_t final_token_width) {
  printf("CLOSE %zu blocks\n", count);
  s->block_close_final_token = final;
  s->blocks_to_close = count - 1;
  s->final_token_width = final_token_width;
  lexer->result_symbol = BLOCK_CLOSE;
  pop_block(s);
}

static bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  uint8_t colons = 0;
  // Because we want to emit a BLOCK_CLOSE token before
  // consuming the `:::` token, we mark the end before advancing
  // to allow us to peek forward.
  lexer->mark_end(lexer);
  while (lexer->lookahead == ':') {
    lexer->advance(lexer, false);
    ++colons;
  }

  if (colons < 3) {
    return false;
  }

  // The context could either be a start or an end token.
  // To figure out which we should do, we search through the entire
  // block stack to find if there's an open block somewhere
  // with the same number of colons.
  // If there is, we should close that one (and all open blocks before),
  // otherwise we start a new div.
  size_t from_top = number_of_blocks_from_top(s, DIV, colons);
  printf("from top: %d\n", from_top);
  if (from_top > 0) {
    close_blocks(s, lexer, from_top, DIV_END, 3);
    dump(s);
    return true;
  } else {
    // We can consume the colons as we start a new div now.
    lexer->mark_end(lexer);
    push_block(s, colons, DIV);
    lexer->result_symbol = DIV_START;
    printf("DIV_START\n");
    dump(s);
    return true;
  }
}

bool tree_sitter_djot_external_scanner_scan(void *payload, TSLexer *lexer,
                                            const bool *valid_symbols) {

  Scanner *s = (Scanner *)payload;

  printf("SCAN\n");
  dump(s);
  printf("? BLOCK_CLOSE %b\n", valid_symbols[BLOCK_CLOSE]);
  printf("? DIV_START %b\n", valid_symbols[DIV_START]);
  printf("? DIV_END %b\n", valid_symbols[DIV_END]);
  printf("current '%c'\n", lexer->lookahead);

  // If we reach oef with open blocks, we should close them all.
  if (lexer->eof(lexer) && s->open_blocks.size > 0) {
    printf("BLOCK_CLOSE eof\n");
    lexer->result_symbol = BLOCK_CLOSE;
    pop_block(s);
    return true;
  }

  if (valid_symbols[BLOCK_CLOSE] && s->blocks_to_close > 0) {
    printf("BLOCK_CLOSE extra\n");
    lexer->result_symbol = BLOCK_CLOSE;
    --s->blocks_to_close;
    pop_block(s);
    return true;
  }
  if (s->blocks_to_close > 0) {
    printf("REJECTED, MUST CLOSE BLOCKS\n");
    // Must close them blocks!
    return false;
  }

  if (s->blocks_to_close == 0 && valid_symbols[s->block_close_final_token]) {
    lexer->result_symbol = s->block_close_final_token;
    s->block_close_final_token = UNUSED;
    while (s->final_token_width--) {
      lexer->advance(lexer, false);
    }
    printf("FINAL TOKEN\n");
    return true;
  }

  if (valid_symbols[DIV_START] || valid_symbols[DIV_END]) {
    return parse_div(s, lexer, valid_symbols);
  }

  return false;
}

void *tree_sitter_djot_external_scanner_create() {
  Scanner *s = (Scanner *)malloc(sizeof(Scanner));
  s->open_blocks.size = 0;
  s->final_token_width = 0;
  s->blocks_to_close = 0;
  s->block_close_final_token = UNUSED;
  // s->open_blocks.items = (Block *)calloc(1, sizeof(Block));
  return s;
}

void tree_sitter_djot_external_scanner_destroy(void *payload) {
  Scanner *s = (Scanner *)payload;
  // printf("%zu", s->open_blocks.size);
  for (size_t i = 0; i < s->open_blocks.size; i++) {
    free(s->open_blocks.items[i]);
  }
  // free(s->open_blocks.items);
  free(s);
}

unsigned tree_sitter_djot_external_scanner_serialize(void *payload,
                                                     char *buffer) {
  Scanner *s = (Scanner *)payload;
  unsigned size = 0;
  buffer[size++] = (char)s->blocks_to_close;
  buffer[size++] = (char)s->block_close_final_token;
  buffer[size++] = (char)s->final_token_width;
  size_t blocks = s->open_blocks.size;
  if (blocks > 0) {
    size_t blocks_size = blocks * sizeof(Block);
    memcpy(&buffer[size], s->open_blocks.items, blocks_size);
    size += blocks_size;
  }

  return size;
}

void tree_sitter_djot_external_scanner_deserialize(void *payload, char *buffer,
                                                   unsigned length) {
  Scanner *s = (Scanner *)payload;
  s->open_blocks.size = 0;
  s->final_token_width = 0;
  s->blocks_to_close = 0;
  s->block_close_final_token = UNUSED;
  if (length > 0) {
    size_t size = 0;
    s->blocks_to_close = (uint8_t)buffer[size++];
    s->block_close_final_token = (TokenType)buffer[size++];
    s->final_token_width = (uint8_t)buffer[size++];

    size_t blocks_size = length - size;
    if (blocks_size > 0) {
      size_t blocks = blocks_size / sizeof(Block);
      memcpy(s->open_blocks.items, &buffer[size], blocks_size);
      s->open_blocks.size = blocks;
    }
  }
}
```

## Closing paragraphs

```js
module.exports = grammar({
  name: "djot",

  // TODO need to escape special characters everywhere
  // maybe we can do this early and automatically skip them in our token logic?

  // Can be directly followed:
  // - Thematic break or fenced block can be followed by paragraph
  // But maybe we already have this?

  // Paragraph can never be interrupted by the start of another block level element
  // Paragraph end: blankline, eof, end of containing block

  // Containing block should automatically close:
  // - Paragraph
  // - Header
  // - Changing list style closes adjacent list of other type
  // - Code block
  // - Raw block
  // - Div

  // Container blocks that can close:
  // - Code block
  // - Raw block
  // - Block quote
  // - Div

  // Link def url can be split into multiple lines

  // Strategy with external scanner:
  // Identify start and end of all blocks.
  //
  // Keep a stack of block level elements to close inside
  // (meaning we need to track all blocks).
  //
  // When something is closed, we should issue a "$._close_block" token
  // that we always match to close all blocks.
  //
  // Need to track paragraphs as well, and paragraphs should close
  // either via $._close_block or blankline or eof.

  extras: (_) => ["\r"],

  // conflicts: ($) => [
  //   [$.paragraph, $.div],
  //   [$._inline_with_newlines, $._close_paragraph],
  // ],

  rules: {
    document: ($) => repeat($._block),

    // Every block contains a newline.
    _block: ($) =>
      choice(
        $._heading,
        // $.list, // Needs external scanner to match indentation!
        // $.pipe_table, // External. Has a caption too that needs to match indent
        // $.footnote, // External, needs to consider indentation level
        $.div,
        // $.codeblock,
        // $.raw_block,
        // $.thematicbreak,
        $.blockquote,
        $.link_reference_definition,
        // $.block_attribute,
        $.paragraph,
        "\n"
      ),

    _heading: ($) =>
      choice(
        $.heading1,
        $.heading2,
        $.heading3,
        $.heading4,
        $.heading5,
        $.heading6
      ),
    heading1: ($) => seq("#", $._gobble_header),
    heading2: ($) => seq(/#{2}/, $._gobble_header),
    heading3: ($) => seq(/#{3}/, $._gobble_header),
    heading4: ($) => seq(/#{4}/, $._gobble_header),
    heading5: ($) => seq(/#{5}/, $._gobble_header),
    heading6: ($) => seq(/#{6}/, $._gobble_header),
    // NOTE because we don't tag the `#` character individually,
    // there's no need to match the beginning `#` of each consecutive line.
    _gobble_header: ($) => seq($._inline_with_newlines, $._eof_or_blankline),

    // I guess we could use an external scanner to allow arbitrary symbols,
    // but this was easier :)
    // div: ($) => choice($._div3, $._div4, $._div5, $._div6, $._div7, $._div8),
    // _div3: ($) => seq(/:{3}/, $._inside_div, /:{3}/),
    // _div4: ($) => seq(/:{4}/, $._inside_div, /:{4}/),
    // _div5: ($) => seq(/:{5}/, $._inside_div, /:{5}/), _div6: ($) => seq(/:{6}/, $._inside_div, /:{6}/),
    // _div7: ($) => seq(/:{7}/, $._inside_div, /:{7}/),
    // _div8: ($) => seq(/:{8}/, $._inside_div, /:{8}/),
    // _inside_div: ($) =>
    //   prec.left(seq(/[ ]*/, optional($.class_name), "\n", repeat($._block))),
    // class_name: (_) => /\w+/,
    div: ($) =>
      seq(
        $.div_marker_start,
        optional($.class_name),
        "\n",
        repeat($._block),
        $._block_close,
        optional(alias($._div_end, $.div_marker_end))
    div_marker_start: ($) =>
      seq($._div_start, optional(seq(/[ ]+/, $.class_name))),
    class_name: (_) => /\w+/,

    // It's fine to let inline gobble up leading `>` for lazy
    // quotes lines.
    blockquote: ($) => seq(">", $._inline_with_newlines, $._eof_or_blankline),

    link_reference_definition: ($) =>
      seq(
        alias($._reference_link_label, $.link_label),
        token.immediate(":"),
        /\s+/,
        $.link_destination,
        $._one_or_two_newlines
      ),
    _reference_link_label: (_) =>
      token(seq("[", token.immediate(/\w+/), token.immediate("]"))),
    link_destination: (_) => /\S+/,

    paragraph: ($) =>
      seq(
        $._inline_with_newlines,
        choice($._eof_or_blankline, $._close_paragraph)
      ),

    _eof_or_blankline: (_) => choice("\0", "\n\n", "\n\0"),
    _one_or_two_newlines: (_) => choice("\0", "\n\n", "\n"),

    _inline: ($) =>
      repeat1(
        choice(
          // $.image,
          // $.autolink,
          // $.verbatim, // Should match ` count
          // $.emphasis,
          // $.strong,
          // $.highlighted,
          // $.superscript,
          // $.subscript,
          // $.insert,
          // $.delete,
          // // Smart punctuation
          // $.math,
          // $.footnote_reference,
          // $.line_break,
          // $.comment,
          // $.symbol,
          // $.raw_inline,
          // $.span,
          // // $.inline_attribute,
          $._link,
          $._text,
          // alias($._text, $.txt)
          " "
        )
      ),
    _inline_with_newlines: ($) => repeat1(prec.left(choice($._inline, /\s/))),

    _link: ($) =>
      choice($.full_reference_link, $.collapsed_reference_link, $.inline_link),

    full_reference_link: ($) => seq($.link_text, $.link_label),
    collapsed_reference_link: ($) => seq($.link_text, token.immediate("[]")),
    inline_link: ($) => seq($.link_text, $.inline_link_destination),

    link_text: ($) => seq("[", $._inline, "]"),

    link_label: ($) => seq("[", $._inline, token.immediate("]")),
    inline_link_destination: (_) => seq("(", /[^\)]+/, ")"),

    _text: (_) => /\S/,
  },

  externals: ($) => [
    $._block_close,
    $._div_start,
    $._div_end,
    $._close_paragraph,

    // Never valid and is used to kill parse branches.
    $._error,
    // Should never be used.
    $._unusued,
  ],
});
```

```c
#include "tree_sitter/parser.h"
#include <assert.h>
#include <stdio.h>
#include <string.h>

// Maybe we should implement a growable stack or something,
// but this is probably fine.
#define STACK_SIZE 512

typedef enum {
  BLOCK_CLOSE,
  DIV_START,
  DIV_END,
  CLOSE_PARAGRAPH,
  ERROR,
  UNUSED
} TokenType;

typedef enum {
  DIV,
} BlockType;

typedef struct {
  // Level can be either indentation or number of opening/ending symbols.
  // Or it may also be unused.
  uint8_t level;
  BlockType type;
} Block;

typedef struct {
  struct {
    size_t size;
    Block *items[STACK_SIZE];
  } open_blocks;

  // How many $._close_block we should output right now?
  uint8_t blocks_to_close;
  // After we have closed all blocks, what symbol should we output?
  TokenType block_close_final_token;
  uint8_t final_token_width;
} Scanner;

static void push_block(Scanner *s, uint8_t level, BlockType type) {
  Block *b = malloc(sizeof(Block));
  b->level = level;
  b->type = type;
  s->open_blocks.items[s->open_blocks.size++] = b;
}

static void pop_block(Scanner *s) {
  if (s->open_blocks.size > 0) {
    // printf("POP\n");
    free(s->open_blocks.items[--s->open_blocks.size]);
  }
}

static bool has_block(Scanner *s) { return s->open_blocks.size > 0; }

static Block *peek_block(Scanner *s) {
  assert(s->open_blocks.size > 0);
  return s->open_blocks.items[s->open_blocks.size - 1];
}

// How many blocks from the top of the stack can we find a matching block?
// If it's directly on the top, returns 1.
// If it cannot be found, returns 0.
static size_t number_of_blocks_from_top(Scanner *s, BlockType type,
                                        uint8_t level) {
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    if (b->type == type && b->level == level) {
      return s->open_blocks.size - i;
    }
  }
  return 0;
}

// Remove 'count' blocks from the stack, freeing them.
static void remove_blocks(Scanner *s, size_t count) {
  while (count-- > 0) {
    pop_block(s);
  }
}

static void dump(Scanner *s) {
  printf("=== Stack size: %zu\n", s->open_blocks.size);
  for (size_t i = 0; i < s->open_blocks.size; ++i) {
    Block *b = s->open_blocks.items[i];
    printf("  %d\n", b->level);
  }
  printf("---\n");
  printf("  blocks_to_close: %d\n", s->blocks_to_close);
  printf("  final_token_width: %d\n", s->final_token_width);
  printf("  block_close_final_token: %u\n", s->block_close_final_token);
  printf("===\n");
}

static void close_blocks(Scanner *s, TSLexer *lexer, size_t count,
                         TokenType final, uint8_t final_token_width) {
  // printf("CLOSE %zu blocks\n", count);
  s->block_close_final_token = final;
  s->blocks_to_close = count - 1;
  s->final_token_width = final_token_width;
  lexer->result_symbol = BLOCK_CLOSE;
  pop_block(s);
}

static bool should_close_paragraph(Scanner *s, TSLexer *lexer,
                                   const bool *valid_symbols) {
  // We're only peeking
  // FIXME combine with parse_div
  lexer->mark_end(lexer);

  uint8_t colons = 0;
  // Because we want to emit a BLOCK_CLOSE token before
  // consuming the `:::` token, we mark the end before advancing
  // to allow us to peek forward.
  lexer->mark_end(lexer);
  while (lexer->lookahead == ':') {
    lexer->advance(lexer, false);
    ++colons;
  }

  if (colons < 3) {
    return false;
  }

  size_t from_top = number_of_blocks_from_top(s, DIV, colons);
  return from_top != 0;
}

static bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  uint8_t colons = 0;
  // Because we want to emit a BLOCK_CLOSE token before
  // consuming the `:::` token, we mark the end before advancing
  // to allow us to peek forward.
  lexer->mark_end(lexer);
  while (lexer->lookahead == ':') {
    lexer->advance(lexer, false);
    ++colons;
  }

  if (colons < 3) {
    return false;
  }

  // The context could either be a start or an end token.
  // To figure out which we should do, we search through the entire
  // block stack to find if there's an open block somewhere
  // with the same number of colons.
  // If there is, we should close that one (and all open blocks before),
  // otherwise we start a new div.
  size_t from_top = number_of_blocks_from_top(s, DIV, colons);
  // printf("from top: %zu\n", from_top);
  if (from_top > 0) {
    close_blocks(s, lexer, from_top, DIV_END, 3);
    // dump(s);
    return true;
  } else {
    // We can consume the colons as we start a new div now.
    lexer->mark_end(lexer);
    push_block(s, colons, DIV);
    lexer->result_symbol = DIV_START;
    // printf("DIV_START\n");
    // dump(s);
    return true;
  }
}

bool tree_sitter_djot_external_scanner_scan(void *payload, TSLexer *lexer,
                                            const bool *valid_symbols) {

  Scanner *s = (Scanner *)payload;

  // printf("SCAN\n");
  // dump(s);
  // printf("? BLOCK_CLOSE %b\n", valid_symbols[BLOCK_CLOSE]);
  // printf("? DIV_START %b\n", valid_symbols[DIV_START]);
  // printf("? DIV_END %b\n", valid_symbols[DIV_END]);
  // printf("? CLOSE_PARAGRAPH %b\n", valid_symbols[CLOSE_PARAGRAPH]);
  // printf("current '%c'\n", lexer->lookahead);

  if (valid_symbols[CLOSE_PARAGRAPH] &&
      should_close_paragraph(s, lexer, valid_symbols)) {
    lexer->result_symbol = CLOSE_PARAGRAPH;
    return true;
  }

  // If we reach oef with open blocks, we should close them all.
  if (lexer->eof(lexer) && s->open_blocks.size > 0) {
    // printf("BLOCK_CLOSE eof\n");
    lexer->result_symbol = BLOCK_CLOSE;
    pop_block(s);
    return true;
  }

  if (valid_symbols[BLOCK_CLOSE] && s->blocks_to_close > 0) {
    // printf("BLOCK_CLOSE extra\n");
    lexer->result_symbol = BLOCK_CLOSE;
    --s->blocks_to_close;
    pop_block(s);
    return true;
  }
  if (s->blocks_to_close > 0) {
    // printf("REJECTED, MUST CLOSE BLOCKS\n");
    // Must close them blocks!
    return false;
  }

  if (s->blocks_to_close == 0 && valid_symbols[s->block_close_final_token]) {
    lexer->result_symbol = s->block_close_final_token;
    s->block_close_final_token = UNUSED;
    while (s->final_token_width--) {
      lexer->advance(lexer, false);
    }
    // printf("FINAL TOKEN\n");
    return true;
  }

  if (valid_symbols[DIV_START] || valid_symbols[DIV_END]) {
    return parse_div(s, lexer, valid_symbols);
  }

  return false;
}

void *tree_sitter_djot_external_scanner_create() {
  Scanner *s = (Scanner *)malloc(sizeof(Scanner));
  s->open_blocks.size = 0;
  s->final_token_width = 0;
  s->blocks_to_close = 0;
  s->block_close_final_token = UNUSED;
  // s->open_blocks.items = (Block *)calloc(1, sizeof(Block));
  return s;
}

void tree_sitter_djot_external_scanner_destroy(void *payload) {
  Scanner *s = (Scanner *)payload;
  // printf("%zu", s->open_blocks.size);
  for (size_t i = 0; i < s->open_blocks.size; i++) {
    free(s->open_blocks.items[i]);
  }
  // free(s->open_blocks.items);
  free(s);
}

unsigned tree_sitter_djot_external_scanner_serialize(void *payload,
                                                     char *buffer) {
  Scanner *s = (Scanner *)payload;
  unsigned size = 0;
  buffer[size++] = (char)s->blocks_to_close;
  buffer[size++] = (char)s->block_close_final_token;
  buffer[size++] = (char)s->final_token_width;
  size_t blocks = s->open_blocks.size;
  if (blocks > 0) {
    size_t blocks_size = blocks * sizeof(Block);
    memcpy(&buffer[size], s->open_blocks.items, blocks_size);
    size += blocks_size;
  }

  return size;
}

void tree_sitter_djot_external_scanner_deserialize(void *payload, char *buffer,
                                                   unsigned length) {
  Scanner *s = (Scanner *)payload;
  s->open_blocks.size = 0;
  s->final_token_width = 0;
  s->blocks_to_close = 0;
  s->block_close_final_token = UNUSED;
  if (length > 0) {
    size_t size = 0;
    s->blocks_to_close = (uint8_t)buffer[size++];
    s->block_close_final_token = (TokenType)buffer[size++];
    s->final_token_width = (uint8_t)buffer[size++];

    size_t blocks_size = length - size;
    if (blocks_size > 0) {
      size_t blocks = blocks_size / sizeof(Block);
      memcpy(s->open_blocks.items, &buffer[size], blocks_size);
      s->open_blocks.size = blocks;
    }
  }
}
```

# Adding highlights

## Using our grammar with Neovim

[external-scanners]: https://tree-sitter.github.io/tree-sitter/creating-parsers#external-scanners
